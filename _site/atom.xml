<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>slug</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2020-06-18T12:46:57-04:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>slugwurth</name>
   <email></email>
 </author>

 
 <entry>
   <title>GPT-2 as a narrative content generator</title>
   <link href="http://localhost:4000/2020/06/18/openai-as-a-narrative-content-generator"/>
   <updated>2020-06-18T00:00:00-04:00</updated>
   <id>http://localhost:4000/2020/06/18/OpenAI-as-a-narrative-content-generator</id>
   <content type="html">&lt;p&gt;I’ve been using something called GPT-2 (General Predictive Text 2) to generate text entries that I share with my party. GPT-2, developed and published by OpenAI, is a natural language model that “predicts” the words and sentences that would follow a user prompt. I’ve trained a simplified version of the model on sci-fi and horror novels found at the Project Gutenberg website.&lt;/p&gt;

&lt;p&gt;I developed this material to support a campaign based in APOF where I’m using a modified Prospero’s Dream and the ACMD outbreak. I pass the word &lt;strong&gt;Caliban&lt;/strong&gt; to my GPT-2 model as a prompt to generate a block of text. By controlling a &lt;em&gt;temperature parameter&lt;/em&gt; in the language model, I can guide the level of percieved insanity of the text’s author&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://txt.fyi/+/662ca8a9/&quot;&gt;Tepid&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://txt.fyi/+/5e4fcd3e/&quot;&gt;Weird&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://txt.fyi/+/fdfd31b7/&quot;&gt;Incoherent&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;while keeping it contextually bound. I have a set of pregens sampled from this spectrum of insanity that I share with players as they examine terminal logs, journals in crew quarters, etc. to fit a descent-into-madness narrative component in my campaign.&lt;/p&gt;

&lt;p&gt;In a similar vein to GPT-2, OAI’s Jukebox is a generative model that makes music &lt;em&gt;with lyrics.&lt;/em&gt; The songs can be haunting by implication and hilarious by content: ‘Classic Pop, in the style of Frank Sinatra’ better known as &lt;a href=&quot;https://soundcloud.com/openai_audio/jukebox-265820820&quot;&gt;“Hot Tub Time”&lt;/a&gt; is a must-listen.&lt;/p&gt;

&lt;p&gt;I’ve included a link below to one of the guides that helped me get everything set up. The process takes some diligence in file naming and tracking but almost all of the heavy lifting has already been done.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Learn more about: &lt;a href=&quot;https://github.com/openai/gpt-2&quot;&gt;GPT-2&lt;/a&gt;, &lt;a href=&quot;https://openai.com/about/&quot;&gt;OpenAI&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;http://www.gutenberg.org/browse/scores/top&quot;&gt;Project Gutenberg&lt;/a&gt; is a great source of plaintext training data&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://soundcloud.com/openai_audio/popular-tracks&quot;&gt;OAI Jukebox&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://medium.com/stasinopoulos.dimitrios/a-beginners-guide-to-training-and-generating-text-using-gpt2-c2f2e1fbd10a&quot;&gt;Getting started&lt;/a&gt; with GPT-2&lt;/p&gt;
&lt;/blockquote&gt;

</content>
 </entry>
 

</feed>
